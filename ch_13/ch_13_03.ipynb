{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245f50bf",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "- 합성곱 신경망(Convolutional Neural Network, 이하 CNN)\n",
    "  - 인간의 시각 처리 방식을 모방한 신경망\n",
    "  - 이미지 처리가 가능하도록 합성곱(Convolution) 연산 도입\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5fe26",
   "metadata": {},
   "source": [
    "- 합성곱층(Convolutional Layer)\n",
    "  - 이미지를 분류하는 데 필요한 특징(Feature) 정보들을 추출하는 역할\n",
    "  - 특징 정보는 필터를 이용해 추출\n",
    "  - 합성곱층에 필터가 적용되면 이미지의 특징들이 추출된 특성 맵이라는 결과를 얻을 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eba002",
   "metadata": {},
   "source": [
    "- 풀링층(Pooling Layer)\n",
    "  - 합성곱층의 출력 데이터(특성 맵)를 입력으로 받아서 출력 데이터인 활성화 맵의 크기를 줄이거나 특정 데이터를 강조하는 용도로 사용\n",
    "  - 풀링층을 처리하는 방법: 최대 풀링, 평균 풀링, 최소 풀링(Min Pooling)\n",
    "  - 정사각행렬의 특정 영역에서 최댓값을 찾거나 평균값을 구하는 방식으로 동작함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7470b",
   "metadata": {},
   "source": [
    "- 완전 연결층(Fully-connected Layer)\n",
    "  - 합성곱층과 풀링층으로 추출한 특징을 분류하는 역할\n",
    "  - CNN은 합성곱층에서 특징만 학습하기 때문에 DFN이나 RNN에 비해 학습해야 하는 가중치의 수가 적어 학습 및 예측이 빠르다는 장점\n",
    "  - 최근에는 CNN의 강력한 예측 성능과 계산상의 효율성을 바탕으로 이미지 뿐만 아니라 시계열 데이터에도 적용해 보는 연구가 활발히 진행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0ee3d",
   "metadata": {},
   "source": [
    "# 워드 임베딩\n",
    "\n",
    "- 워드 임베딩(Word Embedding)\n",
    "  - 단어를 벡터로 표현하는 방법\n",
    "- 원-핫 인코딩(One-hot Encoding)\n",
    "  - N개의 단어를 각각 N차원의 벡터로 표현하는 방식\n",
    "  - 단어가 포함되는 자리엔 1을 넣고 나머지 자리에는 0을 넣는 식\n",
    "    | color | color_red | color_blue | color_green |\n",
    "    | -- | -- | -- | -- |\n",
    "    | red | 1 | 0 | 0 |\n",
    "    | green | 0 | 0 | 1 |\n",
    "    | blue | 0 | 1 | 0 |\n",
    "    | red | 1 | 0 | 0 |\n",
    "- 워드 투 벡터(Word2Vec)\n",
    "  - 비슷한 컨텍스트에 등장하는 단어들은 유사한 의미를 지닌다는 이론에 기반하여 단어를 벡터로 표현해 주는 기법\n",
    "  - 주변 단어를 알면 특정 단어를 유추할 수 있다라는 원리 기반\n",
    "  - 대표적인 모델로는 CBOW와 Skip-gram이 있음\n",
    "  - CBOW\n",
    "    - 전체 컨텍스트로부터 특정 단어를 예측하는 것\n",
    "  - Skip-gram\n",
    "    - 특정 단어로부터 전체 컨텍스트의 분포(확률)를 예측하는 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fcf6e",
   "metadata": {},
   "source": [
    "- 워드 투 벡터 예제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf49596",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mPython 환경 'Python 3.11.12'을(를) 더 이상 사용할 수 없도록 하여 커널을 시작하지 못했습니다. 다른 커널을 선택하거나 Python 환경 목록을 새로 고치는 것이 좋습니다."
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"chased\", \"the\", \"cat\"],\n",
    "    [\"the\", \"cat\", \"and\", \"dog\", \"are\", \"friends\"],\n",
    "]\n",
    "\n",
    "# Word2Vec 모델 학습 (CBOW 방식)\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# 단어 'cat'의 임베딩 벡터 출력\n",
    "cat_vector = model.wv[\"cat\"]\n",
    "print(f\"'cat'의 임베딩 벡터: \\n{cat_vector}\")\n",
    "\n",
    "# 두 단어 사이의 유사도 계산\n",
    "similarity = model.wv.similarity(\"cat\", \"dog\")\n",
    "print(f\"'cat'과 'dog' 사이의 유사도: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592904a",
   "metadata": {},
   "source": [
    "- python 3.11 에서 테스트 진행해야 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1b474",
   "metadata": {},
   "source": [
    "```\n",
    "'cat'의 임베딩 벡터:\n",
    "[-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
    "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
    " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
    " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
    "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
    " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
    "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
    "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
    "  0.00180291  0.01278507]\n",
    "'cat'과 'dog' 사이의 유사도: 0.0111\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4abe0",
   "metadata": {},
   "source": [
    "- TF-IDF\n",
    "  - 단어마다 가중치를 부여하여 단어를 벡터로 변환하는 방법\n",
    "  - TF와 IDF의 곱\n",
    "- TF(Term Frequency)\n",
    "  - 특정 문서에서 특정 단어가 등장하는 횟수\n",
    "- DF(Document Frequency)\n",
    "  - 특정 단어가 등장한 문서의 수\n",
    "- IDF(Inverse Document Frequency)\n",
    "  - DF에 반비례하는 수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcf89e",
   "metadata": {},
   "source": [
    "- Fasttext\n",
    "  - 페이스북에서 개발한 워드 임베딩 방법\n",
    "  - 단어를 벡터로 변환하기 위해 부분 단어(Sub Words)라는 개념을 도입\n",
    "  - N-gram\n",
    "    - 문자열에서 N개의 연속된 요소를 추출하는 방법\n",
    "  - 부분 단어를 사용하면 워드투벡터에서 문제가 되는 모르는 단어 문제를 해결할 수 있기 때문에 임베딩에서 만힝 사용되는 모델 중 하나\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2373a",
   "metadata": {},
   "source": [
    "# 적대적 생성 신경망(GAN)\n",
    "\n",
    "- 적대적 생성 신경망(Generative Adversarial Network, 이하 GAN)\n",
    "  - 2개의 신경망 모델이 서로 경쟁하며 더 나은 결과를 만들어 내는 강화학습\n",
    "  - 이미지 생성 분야에서 뛰어난 성능\n",
    "  - 기존 인공신경망과는 다르게 두 개의 인공신경망이 서로 경쟁하며 학습 진행\n",
    "  - 이를 생성 모델(Generator Model)과 판별 모델(Discriminator Model)이라고 하며, 각각은 서로 다른 목적을 가지고 학습\n",
    "  - 생성 모델\n",
    "    - 주어진 데이터와 최대한 유사한 가짜 데이터를 생성\n",
    "  - 판별 모델\n",
    "    - 진짜 데이터와 가짜 데이터 중 어떤 것이 진짜 데이터인지를 판별\n",
    "  - GAN의 동작 방식은 위조지폐범 판별 문제로 이해 가능\n",
    "  - 위조지폐범은 판별 모델을 속이기 위한 진짜 같은 위조지폐를 만들고 판별 모델은 위조지폐범이 만든 위조 지폐를 찾아내기 위해 서로 경쟁적으로 학습\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-study (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
